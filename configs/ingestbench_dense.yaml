# default logging verbosity (can be overridden with --verbose)
verbose: false

wandb:
  enabled: true
  project: "IngestBench"
  run_name: ${wandb_run_name}
  custom_logging_function: "tasks.ingestbench.custom_functions.log_depth_heatmaps"

chunking:
  strategy: "custom"          
  custom_chunker_module: "tasks.ingestbench.custom_functions"
  custom_chunker_fn: "chunker"
  params: 
    input_path: /dccstor/meghanadhp/projects/IngestBench/data/scientific_research/${gen_cfg}/prose/paraphrased_prose.jsonl

retriever:
  type: dense
  top_k: ${top_k}
  embedding:
    model_name: "Qwen/Qwen3-Embedding-4B"
    device: "cuda"
  vector_store:
    uri: "/dccstor/meghanadhp/projects/rag_recipes/indexes/ingestbench/milvus_${gen_cfg}.db"
    drop_old: true

llm:
  provider: ${llm_provider}       
  model_name: ${model_name}
  temperature: 0
  max_tokens: 2048
  response_format: tasks.ingestbench.custom_functions.OutputSchema
  device: "cuda"
  seed: 42

prompt:
  use_system_message: true
  system_message: |
    You are an assistant for question-answering tasks, applying reasoning or inference when needed.  
    Return only the final factual answer as a Python list of strings.  
    Do not include explanations, reasoning steps, or extra text.  
    If the answer cannot be determined from the context, return ["None"].
    If the question is a yes/no question, return ["Yes"] or ["No"]. 
    If the answer is a numeric value, return it as a string inside a list, e.g., ["42"].
    Otherwise, return a list of entity names that answers the question. Only generate the entity name or names and nothing else.
  template: | 
    {{ system_message }}
    Question: {{ question }}
    Context: {{ context }}
    Answer: 



test_data:
  path: "/dccstor/meghanadhp/projects/IngestBench/data/scientific_research/${gen_cfg}/qas/multihop_v2/benchmark.jsonl"    # path to your eval dataset
  loader: "jsonl"            # "jsonl" or "csv"

preprocessing:
  module: "tasks.ingestbench.custom_functions"
  function: "preprocess_dataset"
  # params: 
  #   num_examples: 10

metrics:
  module: "tasks.ingestbench.custom_functions"
  function: "compute_metrics"

local_dump:
  enabled: true
  output_dir: "outputs/ingestbench/generation"
  filename: ${filename}
  retrieval_dump_path: "outputs/ingestbench/retrieval/${filename}"

defaults:
  gen_cfg: mini
  filename: dense_results.jsonl
  top_k: 5
  wandb_run_name: rag-baseline
  llm_provider: vllm
  model_name: meta-llama/Llama-3.1-8B-Instruct